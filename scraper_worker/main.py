# scraper_worker/main.py (FULL VERSION)
import requests
import json
from datetime import datetime, date, timedelta, timezone
from concurrent.futures import ThreadPoolExecutor, as_completed
from threading import Lock
import logging
import os
import time


import sys


# Models import
from models_standalone import (
    Route, Journey, PriceHistory, PriceAlert, 
    CompanyRoute, User, get_session
)

# Logging setup
#logging.basicConfig(
#    level=logging.INFO,
#    format='%(asctime)s - %(levelname)s - %(message)s'
#)
#logger = logging.getLogger(__name__)

# ScrapingBee API Key
API_KEY = os.getenv('SCRAPINGBEE_API_KEY', '1BK95SF6JYACP830LL46SNQWJZZYZVMF6QS04DHBLE6QAIZNPVGO30O5CRN9HUMMNX6LC6FML1KJSDOE')





# Logging konfig√ºrasyonu - STDOUT'a yaz (DigitalOcean loglarƒ± g√∂rebilmek i√ßin)
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(sys.stdout)  # STDOUT'a yaz
    ]
)
logger = logging.getLogger(__name__)


class ScrapingBeeMonitor:
    """ScrapingBee ban/block tespiti"""
    
    def __init__(self):
        self.total_requests = 0
        self.failed_requests = 0
        self.blocked_requests = 0
        self.rate_limited = 0
    
    def is_blocked_response(self, response):
        """Response blocked/banned mi kontrol et"""
        
        # Status code kontrol√º
        if response.status_code in [403, 429]:  # Forbidden, Too Many Requests
            return True
        
        # ScrapingBee specific errors
        if response.status_code == 422:  # ScrapingBee error
            return True
        
        # Response body kontrol√º
        try:
            data = response.json()
            
            # ScrapingBee error mesajlarƒ±
            if 'error' in data:
                error_msg = str(data['error']).lower()
                if any(keyword in error_msg for keyword in ['blocked', 'banned', 'captcha', 'rate limit']):
                    return True
        except:
            pass
        
        return False
    
    def record_request(self, response):
        """Request sonucunu kaydet"""
        self.total_requests += 1
        
        if response.status_code != 200:
            self.failed_requests += 1
        
        if self.is_blocked_response(response):
            self.blocked_requests += 1
            logger.warning(f"‚ö†Ô∏è  BLOCKED RESPONSE: Status {response.status_code}")
        
        if response.status_code == 429:
            self.rate_limited += 1
    
    def get_block_rate(self):
        """Block oranƒ±nƒ± hesapla"""
        if self.total_requests == 0:
            return 0
        return (self.blocked_requests / self.total_requests) * 100
    
    def should_alert(self):
        """Alert g√∂nderilmeli mi?"""
        # %20'den fazla block varsa alert
        return self.get_block_rate() > 20 or self.rate_limited > 5




class ObiletScraper:
    def __init__(self, max_workers=4, max_retries=10, batch_size=500):
        self.max_workers = max_workers
        self.max_retries = max_retries
        self.batch_size = batch_size
        
        # Buffers - memory'de biriktir
        self.scraped_data = []  # T√ºm scrape edilen journeys
        self.lock = Lock()
        
        # Statistics
        self.total_routes = 0
        self.completed_routes = 0
        self.failed_routes = 0
        self.total_journeys = 0
        self.ban_monitor = ScrapingBeeMonitor()
        
    def get_active_routes(self):
        """Database'den aktif route'larƒ± √ßek"""
        session = get_session()
        try:
            routes = session.query(Route).filter_by(is_active=True).all()
            logger.info(f"üìã Found {len(routes)} active routes in database")
            return routes
        except Exception as e:
            logger.error(f"‚ùå Database error: {e}")
            return []
        finally:
            session.close()
    
    def get_obilet_journeys(self, origin_id, destination_id, date_str):
        """
        Obilet JSON endpoint'inden seferleri √ßeker (ScrapingBee ile)
        """
        url = f"https://www.obilet.com/json/journeys/{origin_id}-{destination_id}/{date_str}"
        print(url)
        
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
            'Accept': 'application/json',
            'Referer': 'https://www.obilet.com/'
        }
        
        try:
            response = requests.post(
                "https://app.scrapingbee.com/api/v1/",
                params={
                    "api_key": API_KEY,
                    "url": url,
                    "country_code": "tr",
                    "render_js": False,
                    "premium_proxy": False,
                },
                headers=headers,
                timeout=30
            )

            self.ban_monitor.record_request(response)

            if response.status_code != 200:
                logger.error(f"‚ùå ScrapingBee error: {response.status_code}")
                return None  # ‚ùå API hatasƒ± - None d√∂nd√ºr
            
            data = response.json()
            journeys = data.get('journeys', [])
            
            parsed_journeys = []
            
            for j in journeys:
                journey = j.get('journey', {})
                
                parsed = {
                    'id': j.get('id'),
                    'partner_id': j.get('partner-id'),
                    'partner_name': j.get('partner-name'),
                    'bus_type': j.get('bus-type'),
                    'total_seats': j.get('total-seats'),
                    'available_seats': j.get('available-seats'),
                    
                    # Journey detaylarƒ±
                    'origin': journey.get('origin'),
                    'destination': journey.get('destination'),
                    'departure': journey.get('departure'),
                    'arrival': journey.get('arrival'),
                    'duration': 0,
                    
                    # Fiyat
                    'original_price': journey.get('original-price'),
                    'internet_price': journey.get('internet-price'),
                    'currency': journey.get('currency'),
                    
                    # Diƒüer bilgiler
                    'bus_name': journey.get('bus-name'),
                    'peron_no': journey.get('peron-no'),
                    
                    # √ñzellikler
                    'features': [],
                    
                    # Duraklar
                    'stops': [
                        {
                            'name': stop.get('name'),
                            'time': stop.get('time'),
                            'is_origin': stop.get('is-origin'),
                            'is_destination': stop.get('is-destination')
                        }
                        for stop in journey.get('stops', [])
                    ],
                    
                    # Rating
                    'partner_rating': j.get('partner-rating'),
                    'partner_route_rating': j.get('partner-route-rating'),
                }
                
                parsed_journeys.append(parsed)
            
            # ‚úÖ API ba≈üarƒ±lƒ± - bo≈ü liste bile olsa liste d√∂nd√ºr
            return parsed_journeys
            
        except requests.exceptions.RequestException as e:
            logger.error(f"‚ùå Request error: {e}")
            return None  # ‚ùå Network hatasƒ±
        except json.JSONDecodeError as e:
            logger.error(f"‚ùå JSON parse error: {e}")
            return None  # ‚ùå Parse hatasƒ±
        except Exception as e:
            logger.error(f"‚ùå Unexpected error: {e}")
            return None  # ‚ùå Beklenmeyen hata


    def send_ban_alert(self):
        """Ban/block alerti g√∂nder"""
        from models_standalone import Notification, User
        
        session = get_session()
        
        try:
            # Admin kullanƒ±cƒ±larƒ± bul
            admins = session.query(User).filter_by(role='admin', is_active=True).all()
            
            for admin in admins:
                notification = Notification(
                    user_id=admin.id,
                    title="‚ö†Ô∏è ScrapingBee Blocking Detected!",
                    message=f"""
Scraper is experiencing high block rates:
- Total Requests: {self.ban_monitor.total_requests}
- Blocked: {self.ban_monitor.blocked_requests}
- Block Rate: {self.ban_monitor.get_block_rate():.1f}%
- Rate Limited: {self.ban_monitor.rate_limited}

Action may be required!
                    """.strip(),
                    notification_type='error',
                    priority='high',
                    is_read=False
                )
                session.add(notification)
            
            session.commit()
            logger.error(f"üö® BAN ALERT SENT - Block rate: {self.ban_monitor.get_block_rate():.1f}%")
            
        except Exception as e:
            logger.error(f"‚ùå Failed to send ban alert: {e}")
        finally:
            session.close()


    def buffer_journeys(self, route, journeys, date_str):
        """Thread-safe buffer'a ekle"""
        with self.lock:
            for journey in journeys:
                # Route bilgisini de ekle
                journey['route_id'] = route.id
                journey['route_name'] = route.route_name or f"{route.origin_city_name} - {route.destination_city_name}"
                journey['scraped_date'] = date_str
                journey['scraped_at'] = datetime.utcnow().isoformat()
                
                self.scraped_data.append(journey)
            
            self.total_journeys += len(journeys)
    
    def parse_datetime_safe(self, datetime_str):
        """
        Datetime string'i parse et, timezone ekle
        """
        if not datetime_str:
            return None
        
        try:
            # ISO format parse et
            dt = datetime.fromisoformat(datetime_str.replace('Z', '+00:00'))
            
            # Timezone yoksa UTC ekle
            if dt.tzinfo is None:
                from datetime import timezone
                dt = dt.replace(tzinfo=timezone.utc)
            
            return dt
        except:
            try:
                # Alternatif format
                dt = datetime.strptime(datetime_str, '%Y-%m-%dT%H:%M:%S')
                from datetime import timezone
                dt = dt.replace(tzinfo=timezone.utc)
                return dt
            except:
                logger.warning(f"‚ö†Ô∏è  Could not parse datetime: {datetime_str}")
                return None

    def filter_journeys_by_date(self, journeys, target_date):
        """
        Sadece target_date'e ait journey'leri filtrele
        Ertesi g√ºn√ºn seferlerini exclude et
        """
        filtered = []
        excluded_count = 0
        
        for journey in journeys:
            departure_str = journey.get('departure')
            
            if not departure_str:
                continue
            
            # Parse et
            departure_dt = self.parse_datetime_safe(departure_str)
            
            if not departure_dt:
                continue
            
            # Tarihi kar≈üƒ±la≈ütƒ±r (sadece date kƒ±smƒ±)
            journey_date = departure_dt.date()
            
            if journey_date == target_date:
                filtered.append(journey)
            else:
                excluded_count += 1
                logger.debug(f"  ‚è≠Ô∏è  Excluded: {journey.get('partner_name')} @ {departure_dt} (different date)")
        
        if excluded_count > 0:
            logger.info(f"  üìÖ Filtered: {len(filtered)} kept, {excluded_count} excluded (wrong date)")
        
        return filtered

    def scrape_route_with_retry(self, route, date_str):
        """
        Tek bir route i√ßin scraping yap (G√úNCELLENMƒ∞≈û)
        """
        route_name = route.route_name or f"{route.origin_city_name} ‚Üí {route.destination_city_name}"
        logger.warning(f"‚ö†Ô∏è  {route_name}: TEKRAR DENENƒ∞YOR!!!!!!!")
        
        for attempt in range(self.max_retries):
            try:
                # Obilet'ten veri √ßek
                journeys = self.get_obilet_journeys(
                    origin_id=route.origin_obilet_id,
                    destination_id=route.destination_obilet_id,
                    date_str=date_str
                )
                
                # ‚ùå API hatasƒ± (None d√∂nd√º) - retry yapƒ±lacak
                if journeys is None:
                    if attempt == self.max_retries - 1:
                        logger.error(f"‚ùå {route_name}: API failed after {self.max_retries} attempts")
                        with self.lock:
                            self.failed_routes += 1
                        return {'success': False, 'api_error': True}
                    else:
                        logger.warning(f"‚ö†Ô∏è  {route_name}: API error, retrying... (attempt {attempt + 1}/{self.max_retries})")
                        time.sleep(2 ** attempt)  # Exponential backoff
                        continue
                
                # ‚úÖ API ba≈üarƒ±lƒ± (bo≈ü liste de olabilir)
                if journeys:
                    # Target date objesini olu≈ütur
                    target_date = datetime.strptime(date_str, '%Y-%m-%d').date()
                    
                    # ‚ùó SADECE O G√úN√úN SEFERLERƒ∞Nƒ∞ Fƒ∞LTRELE
                    filtered_journeys = self.filter_journeys_by_date(journeys, target_date)
                    
                    if filtered_journeys:
                        # Buffer'a ekle
                        self.buffer_journeys(route, filtered_journeys, date_str)
                        
                        with self.lock:
                            self.completed_routes += 1
                        
                        logger.info(f"‚úÖ [{self.completed_routes}/{self.total_routes}] {route_name}: {len(filtered_journeys)} journeys (filtered from {len(journeys)})")
                        return {'success': True, 'count': len(filtered_journeys)}
                    else:
                        logger.warning(f"‚ö†Ô∏è  {route_name}: All journeys excluded (wrong date)")
                        with self.lock:
                            self.completed_routes += 1
                        return {'success': True, 'count': 0}
                else:
                    # ‚úÖ API ba≈üarƒ±lƒ± ama bo≈ü liste (sefer yok)
                    logger.warning(f"‚ö†Ô∏è  {route_name}: No journeys found (API returned empty)")
                    with self.lock:
                        self.completed_routes += 1
                    return {'success': True, 'count': 0}
                
            except Exception as e:
                if attempt == self.max_retries - 1:
                    logger.error(f"‚ùå {route_name} failed after {self.max_retries} attempts: {e}")
                    with self.lock:
                        self.failed_routes += 1
                    return {'success': False, 'error': str(e)}
                
                wait_time = 2 ** attempt
                logger.warning(f"‚ü≥ {route_name} attempt {attempt + 1}/{self.max_retries} failed, retrying in {wait_time}s...")
                time.sleep(wait_time)

    def get_unique_key(self, journey_data):
        """
        Journey'yi benzersiz ≈üekilde tanƒ±mlayan key
        (route_id, departure_time_iso, partner_id)
        """
        # Departure'ƒ± parse et ve isoformat'a √ßevir (DB ile e≈üle≈ümesi i√ßin)
        departure_dt = self.parse_datetime_safe(journey_data.get('departure'))
        departure_iso = departure_dt.isoformat() if departure_dt else None
        
        return (
            journey_data['route_id'],
            departure_iso,
            journey_data.get('partner_id')
        )
    

    def create_journey_object(self, data):
        """
        Scraped data'dan Journey objesi olu≈ütur (G√úNCELLENMƒ∞≈û)
        """
        # ‚ùó YENƒ∞ PARSE FONKSƒ∞YONU KULLAN
        departure_dt = self.parse_datetime_safe(data.get('departure'))
        arrival_dt = self.parse_datetime_safe(data.get('arrival'))
        
        # Occupancy rate hesapla
        occupancy_rate = None
        if data.get('total_seats') and data['total_seats'] > 0:
            occupied = data['total_seats'] - data.get('available_seats', 0)
            occupancy_rate = round((occupied / data['total_seats']) * 100, 2)
        
        return Journey(
            route_id=data['route_id'],
            company_name=data.get('partner_name', 'Unknown'),
            obilet_partner_id=data.get('partner_id'),
            departure_time=departure_dt,
            arrival_time=arrival_dt,
            duration=data.get('duration'),
            original_price=data.get('original_price'),
            internet_price=data.get('internet_price'),
            currency=data.get('currency', 'TRY'),
            total_seats=data.get('total_seats'),
            available_seats=data.get('available_seats', 0),
            occupancy_rate=occupancy_rate,
            bus_type=data.get('bus_type'),
            bus_plate="",
            #bus_plate=data.get('bus_name'),
            has_wifi='Wifi' in data.get('features', []) or 'Wi-Fi' in data.get('features', []),
            has_usb='USB' in data.get('features', []),
            has_tv='TV' in data.get('features', []) or 'Ekran' in data.get('features', []),
            has_socket='Priz' in data.get('features', []) or '≈ûarj' in data.get('features', []),
            obilet_journey_id=data.get('id'),
            is_active=True
        )

    def sync_journeys_for_route(self, route_id, new_journeys_data, target_date):
        """
        Bir route i√ßin journey'leri senkronize et - HARD DELETE
        - API'den gelen g√ºncel data ile DB'deki journeys'leri kar≈üƒ±la≈ütƒ±r
        - API'de olmayan herkesi GER√áEKTEN Sƒ∞L (hard delete)
        - is_active kullanmƒ±yoruz artƒ±k
        """
        session = get_session()
        
        try:
            # 1. DB'den o route i√ßin T√úM journeys'i √ßek (is_active yok artƒ±k)
            existing_journeys = session.query(Journey).filter(
                Journey.route_id == route_id
            ).all()
            
            # Existing journeys'i obilet_journey_id'ye g√∂re dict'e √ßevir
            existing_dict = {}
            for j in existing_journeys:
                if j.obilet_journey_id:
                    existing_dict[str(j.obilet_journey_id)] = j
            
            # New journeys'i obilet_journey_id'ye g√∂re dict'e √ßevir
            new_dict = {}
            for data in new_journeys_data:
                journey_id = data.get('id')
                if journey_id:
                    new_dict[str(journey_id)] = data
            
            existing_ids = set(existing_dict.keys())
            new_ids = set(new_dict.keys())
            
            logger.info(f"  üîç Debug: Existing IDs count: {len(existing_ids)}, New IDs count: {len(new_ids)}")
            
            # 2. Silinecekler - API'de olmayan herkesi GER√áEKTEN Sƒ∞L
            to_delete_ids = existing_ids - new_ids
            deleted_count = 0
            
            if to_delete_ids:
                logger.info(f"  üóëÔ∏è  Will DELETE {len(to_delete_ids)} journeys (hard delete)")
            
            for journey_id in to_delete_ids:
                journey = existing_dict[journey_id]
                session.delete(journey)  # üóëÔ∏è HARD DELETE
                deleted_count += 1
                logger.info(f"  üóëÔ∏è  Deleted: {journey.company_name} @ {journey.departure_time.strftime('%Y-%m-%d %H:%M') if journey.departure_time else 'N/A'} (ID: {journey_id})")
            
            # 3. G√ºncellenecekler
            to_update_ids = existing_ids & new_ids
            updated_count = 0
            price_changes = []
            
            for journey_id in to_update_ids:
                existing_journey = existing_dict[journey_id]
                new_data = new_dict[journey_id]
                
                new_price = new_data.get('internet_price')
                new_seats = new_data.get('available_seats', 0)
                
                old_price = existing_journey.internet_price
                
                # üîß Float/Decimal sorunu - hepsini float yap
                if old_price is not None:
                    old_price = float(old_price)
                if new_price is not None:
                    new_price = float(new_price)
                
                price_changed = old_price and new_price and old_price != new_price
                seats_changed = existing_journey.available_seats != new_seats
                
                if price_changed or seats_changed:
                    existing_journey.internet_price = new_price
                    existing_journey.original_price = new_data.get('original_price')
                    existing_journey.available_seats = new_seats
                    existing_journey.total_seats = new_data.get('total_seats')
                    
                    if new_data.get('total_seats') and new_data['total_seats'] > 0:
                        occupied = new_data['total_seats'] - new_seats
                        existing_journey.occupancy_rate = round((occupied / new_data['total_seats']) * 100, 2)
                    
                    existing_journey.scraped_at = datetime.utcnow()
                    updated_count += 1
                    
                    if price_changed:
                        change_pct = ((new_price - old_price) / old_price) * 100
                        price_changes.append({
                            'journey': existing_journey,
                            'old_price': old_price,
                            'new_price': new_price,
                            'change_pct': change_pct
                        })
                        logger.info(f"  üí∞ Price changed: {existing_journey.company_name} @ {existing_journey.departure_time.strftime('%H:%M') if existing_journey.departure_time else 'N/A'} | {old_price} ‚Üí {new_price} TRY ({change_pct:+.1f}%)")
            
            # 4. Eklenecekler - Ger√ßekten yeni olanlarƒ± ekle
            to_insert_ids = new_ids - existing_ids
            inserted_journeys = []
            
            if to_insert_ids:
                logger.info(f"  ‚ûï Will insert {len(to_insert_ids)} new journeys")
            
            for journey_id in to_insert_ids:
                new_data = new_dict[journey_id]
                journey_obj = self.create_journey_object(new_data)
                session.add(journey_obj)
                inserted_journeys.append(journey_obj)
                logger.info(f"  ‚ûï New journey: {journey_obj.company_name} @ {journey_obj.departure_time.strftime('%H:%M') if journey_obj.departure_time else 'N/A'} | {journey_obj.internet_price} TRY (ID: {journey_id})")
            
            session.commit()
            
            # 5. Alert olu≈ütur
            self.create_alerts_for_changes(
                session=session,
                route_id=route_id,
                price_changes=price_changes,
                new_journeys=inserted_journeys,
                target_date=target_date
            )
            
            logger.info(f"  üìä Route {route_id} sync: {len(to_insert_ids)} inserted, {updated_count} updated, {deleted_count} deleted")
            
            return {
                'inserted': len(to_insert_ids),
                'updated': updated_count,
                'deleted': deleted_count,
                'price_changes': len(price_changes)
            }
            
        except Exception as e:
            session.rollback()
            logger.error(f"‚ùå Sync error for route {route_id}: {e}")
            raise
        finally:
            session.close()
        
    def create_alerts_for_changes(self, session, route_id, price_changes, new_journeys, target_date):
        """
        Fiyat deƒüi≈üiklikleri ve yeni seferler i√ßin alert olu≈ütur
        """
        try:
            # Bu route'u takip eden firmalarƒ± bul
            company_routes = session.query(CompanyRoute).filter(
                CompanyRoute.route_id == route_id,
                CompanyRoute.is_active == True
            ).all()
            
            if not company_routes:
                return
            
            # Her firma i√ßin alert olu≈ütur
            for cr in company_routes:
                user = cr.user
                
                # Fiyat deƒüi≈üikliƒüi alertleri
                if cr.alert_on_price_change:
                    for change in price_changes:
                        # Threshold kontrol√º
                        if abs(change['change_pct']) >= float(cr.alert_threshold_percentage or 0):
                            
                            alert_type = 'price_drop' if change['change_pct'] < 0 else 'price_increase'
                            
                            alert = PriceAlert(
                                user_id=user.id,
                                route_id=route_id,
                                alert_type=alert_type,
                                title=f"{'Fiyat D√º≈üt√º' if alert_type == 'price_drop' else 'Fiyat Arttƒ±'}: {change['journey'].company_name}",
                                message=f"{change['journey'].company_name} firmasƒ±nƒ±n {change['journey'].departure_time.strftime('%H:%M') if change['journey'].departure_time else 'N/A'} seferinde fiyat {change['old_price']} TRY'den {change['new_price']} TRY'ye deƒüi≈üti ({change['change_pct']:+.1f}%)",
                                competitor_name=change['journey'].company_name,
                                old_price=change['old_price'],
                                new_price=change['new_price'],
                                price_change_percentage=change['change_pct'],
                                departure_date=target_date,
                                priority='high' if abs(change['change_pct']) > 20 else 'medium',
                                is_read=False,
                                is_sent=False
                            )
                            session.add(alert)
                            logger.info(f"    üîî Alert created for user {user.company_name}: Price change")
                
                # Yeni sefer alertleri
                for new_journey in new_journeys:
                    # En d√º≈ü√ºk fiyatlƒ± mƒ± kontrol et
                    min_price_journey = session.query(Journey).filter(
                        Journey.route_id == route_id,
                        Journey.departure_time >= target_date,
                        Journey.departure_time < target_date + timedelta(days=1),
                        Journey.is_active == True
                    ).order_by(Journey.internet_price.asc()).first()
                    
                    is_lowest_price = (min_price_journey and 
                                      new_journey.internet_price == min_price_journey.internet_price)
                    
                    alert = PriceAlert(
                        user_id=user.id,
                        route_id=route_id,
                        alert_type='new_journey',
                        title=f"Yeni Sefer Eklendi: {new_journey.company_name}",
                        message=f"{new_journey.company_name} firmasƒ± {new_journey.departure_time.strftime('%H:%M') if new_journey.departure_time else 'N/A'} seferini ekledi. Fiyat: {new_journey.internet_price} TRY" + (" - EN D√ú≈û√úK Fƒ∞YAT! üéâ" if is_lowest_price else ""),
                        competitor_name=new_journey.company_name,
                        new_price=new_journey.internet_price,
                        departure_date=target_date,
                        priority='high' if is_lowest_price else 'low',
                        is_read=False,
                        is_sent=False
                    )
                    session.add(alert)
                    logger.info(f"    üîî Alert created for user {user.company_name}: New journey")
            
            session.commit()
            
        except Exception as e:
            logger.error(f"‚ùå Alert creation error: {e}")
    
    def insert_price_history_for_route(self, route_journeys, target_date):
        """
        Bir route i√ßin Price History ekle
        """
        session = get_session()
        
        try:
            price_records = []
            
            for data in route_journeys:
                departure_dt = None
                if data.get('departure'):
                    try:
                        departure_dt = datetime.fromisoformat(data['departure'].replace('Z', '+00:00'))
                    except:
                        pass
                
                occupancy_rate = None
                if data.get('total_seats') and data['total_seats'] > 0:
                    occupied = data['total_seats'] - data.get('available_seats', 0)
                    occupancy_rate = round((occupied / data['total_seats']) * 100, 2)
                
                days_before = (target_date - date.today()).days if target_date else 0
                
                price_hist = PriceHistory(
                    route_id=data['route_id'],
                    company_name=data.get('partner_name', 'Unknown'),
                    obilet_partner_id=data.get('partner_id'),
                    price=data.get('internet_price'),
                    currency=data.get('currency', 'TRY'),
                    departure_date=target_date,
                    days_before_departure=days_before,
                    available_seats=data.get('available_seats', 0),
                    total_seats=data.get('total_seats'),
                    occupancy_rate=occupancy_rate
                )
                
                price_records.append(price_hist)
            
            if price_records:
                session.bulk_save_objects(price_records)
                session.commit()
                logger.info(f"    üíæ Price History: {len(price_records)} records added")
            
        except Exception as e:
            session.rollback()
            logger.error(f"‚ùå Price History error: {e}")
        finally:
            session.close()
    
    def cleanup_old_data(self, days_to_keep=0):
        """
        Eski verileri temizle
        - Journey: is_active=False ve eski olanlarƒ± sil
        - PriceHistory: X g√ºnden eski olanlarƒ± sil
        """
        session = get_session()
        cutoff_date = datetime.utcnow() - timedelta(days=days_to_keep)
        
        try:
            # Journey'leri temizle (soft deleted + eski)
            deleted_journeys = session.query(Journey).filter(
                Journey.is_active == False,
                Journey.scraped_at < cutoff_date
            ).delete()
            
            # Price History temizle
            deleted_price_history = session.query(PriceHistory).filter(
                PriceHistory.recorded_at < cutoff_date
            ).delete()
            
            session.commit()
            
            logger.info(f"üßπ Cleanup: {deleted_journeys} old journeys, {deleted_price_history} old price records deleted")
            
        except Exception as e:
            session.rollback()
            logger.error(f"‚ùå Cleanup error: {e}")
        finally:
            session.close()
    
    def run(self, target_date=None, cleanup_old_data=False):
        """
        Ana scraping + sync fonksiyonu
        """
        logger.info("=" * 80)
        logger.info("üöÄ Obilet Scraper Starting...")
        logger.info("=" * 80)
        
        start_time = time.time()
        
        # Target date (default: bug√ºn)
        if not target_date:
            target_date = date.today()
            print(target_date)
        
        date_str = target_date.strftime('%Y-%m-%d')
        logger.info(f"üìÖ Target Date: {date_str}")
        
        # Eski verileri temizle (opsiyonel)
        if cleanup_old_data:
            logger.info("\nüßπ Cleaning up old data...")
            self.cleanup_old_data(days_to_keep=30)
            logger.info("")
        
        # Database'den route'larƒ± √ßek
        routes = self.get_active_routes()
        
        if not routes:
            logger.error("‚ùå No active routes found in database!")
            return
        
        self.total_routes = len(routes)
        logger.info(f"üìä Total Routes: {self.total_routes}")
        logger.info(f"‚öôÔ∏è  Max Workers: {self.max_workers}")
        logger.info("-" * 80)
        
        # Statistics
        total_inserted = 0
        total_updated = 0
        total_deleted = 0
        total_price_changes = 0
        
        # Her route i√ßin scrape et
        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            # Future'larƒ± dictionary'de tut
            future_to_route = {
                executor.submit(self.scrape_route_with_retry, route, date_str): route
                for route in routes
            }
            
            for future in as_completed(future_to_route):
                route = future_to_route[future]
                
                try:
                    # Scraping sonucu
                    result = future.result()
                    
                    # ‚úÖ API ba≈üarƒ±lƒ± - bo≈ü liste de olabilir
                    if result['success']:
                        # Bu route i√ßin scraped journeys'i al (bo≈ü liste olabilir)
                        route_journeys = [
                            j for j in self.scraped_data 
                            if j['route_id'] == route.id
                        ]
                        
                        # Sync yap - API ba≈üarƒ±lƒ±ysa bo≈ü bile olsa sync et
                        logger.info(f"\nüîÑ Syncing route {route.id}: {route.route_name or 'N/A'} ({len(route_journeys)} journeys)")
                        sync_result = self.sync_journeys_for_route(
                            route_id=route.id,
                            new_journeys_data=route_journeys,
                            target_date=target_date
                        )
                        
                        total_inserted += sync_result['inserted']
                        total_updated += sync_result['updated']
                        total_deleted += sync_result['deleted']
                        total_price_changes += sync_result['price_changes']
                        
                        # Price History ekle (sadece veri varsa)
                        if route_journeys:
                            self.insert_price_history_for_route(route_journeys, target_date)
                    else:
                        # ‚ùå API hatasƒ± - eski verileri koru (sync yapma)
                        logger.warning(f"‚ö†Ô∏è  Route {route.id} skipped sync (API error - preserving old data)")
                    
                except Exception as e:
                    logger.error(f"‚ùå Error processing route {route.id}: {e}")
        
        if self.ban_monitor.should_alert():
            logger.error("üö® HIGH BLOCK RATE DETECTED!")
            self.send_ban_alert()


        # Final statistics
        elapsed = time.time() - start_time
        
        logger.info("=" * 80)
        logger.info("‚úÖ Scraper Completed!")
        logger.info(f"   Duration: {elapsed:.1f}s")
        logger.info(f"   Routes Processed: {self.completed_routes}/{self.total_routes}")
        logger.info(f"   Routes Failed: {self.failed_routes}")
        logger.info(f"   Total Journeys Scraped: {self.total_journeys}")
        logger.info("")
        logger.info("   üìä Database Changes:")
        logger.info(f"      Inserted: {total_inserted}")
        logger.info(f"      Updated: {total_updated}")
        logger.info(f"      Deleted: {total_deleted}")
        logger.info(f"      Price Changes: {total_price_changes}")
        logger.info("=" * 80)
        
        return self.scraped_data


if __name__ == '__main__':
    # Database URL check
    if not os.getenv('DATABASE_URL', 'postgresql://doadmin:AVNS_zd8YbZpiFc5dU9HRIc3@db-postgresql-fra1-91466-do-user-30609413-0.i.db.ondigitalocean.com:25060/defaultdb?sslmode=require'):
        logger.error("‚ùå DATABASE_URL environment variable not set!")
        logger.info("Usage: export DATABASE_URL='postgresql://user:pass@host:5432/dbname'")
        exit(1)
    
    # Scraper √ßalƒ±≈ütƒ±r
    scraper = ObiletScraper(
        max_workers=4,
        max_retries=10,
        batch_size=500
    )
    
    # Bug√ºn i√ßin scrape et
    # cleanup_old_data=True ‚Üí 30 g√ºnden eski verileri sil
    scraped_data = scraper.run(cleanup_old_data=True)
    
    logger.info("\nüéâ All operations completed successfully!")
